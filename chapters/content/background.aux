\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{background}{{2}{11}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Genomics}{11}{section.2.1}\protected@file@percent }
\citation{nussbaum_mcinnes_willard_2016}
\citation{nussbaum_mcinnes_willard_2016}
\citation{nussbaum_mcinnes_willard_2016}
\citation{nussbaum_mcinnes_willard_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Genetic Variation, Polymorphisms and Sequencing}{12}{subsection.2.1.1}\protected@file@percent }
\citation{nussbaum_mcinnes_willard_2016}
\citation{muts}
\citation{muts}
\citation{nussbaum_mcinnes_willard_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces {\bf  Some types of nucleotide mutations\cite  {muts}} This shows the three most common variations. The SNP is on the left, then an insertion where a sequence is inserted, and the deletion where the opposite occurs}}{13}{figure.2.1}\protected@file@percent }
\newlabel{genfig2}{{2.1}{13}{{\bf Some types of nucleotide mutations\cite {muts}} This shows the three most common variations. The SNP is on the left, then an insertion where a sequence is inserted, and the deletion where the opposite occurs}{figure.2.1}{}}
\citation{nussbaum_mcinnes_willard_2016}
\citation{nussbaum_mcinnes_willard_2016}
\citation{ldfig}
\citation{ldfig}
\citation{nussbaum_mcinnes_willard_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Linkage (Dis)Equilibrium and Missing Heritability}{14}{subsection.2.1.2}\protected@file@percent }
\newlabel{missHerit}{{2.1.2}{14}{Linkage (Dis)Equilibrium and Missing Heritability}{subsection.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces {\bf  Example of LD plots of marker rs17848945 \cite  {ldfig}} Image a) gives the LD plot of the given SNP and the $R^2$ value of close SNPs. Image b) shows the LD block plot for further correlation analysis}}{15}{figure.2.2}\protected@file@percent }
\newlabel{genfig3}{{2.2}{15}{{\bf Example of LD plots of marker rs17848945 \cite {ldfig}} Image a) gives the LD plot of the given SNP and the $R^2$ value of close SNPs. Image b) shows the LD block plot for further correlation analysis}{figure.2.2}{}}
\citation{nussbaum_mcinnes_willard_2016}
\citation{nussbaum_mcinnes_willard_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Genome-Wide Association Study}{16}{subsection.2.1.3}\protected@file@percent }
\newlabel{GWAS}{{2.1.3}{16}{Genome-Wide Association Study}{subsection.2.1.3}{}}
\citation{pers}
\citation{Torkamani2018}
\citation{Consortium2009}
\citation{Consortium2009}
\citation{Consortium2009}
\citation{Marquez-Luna375337}
\citation{ldpreeed}
\citation{Marquez-Luna375337}
\citation{Marquez-Luna375337}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Polygenic Risk Scores}{17}{subsection.2.1.4}\protected@file@percent }
\newlabel{PRS}{{2.1.4}{17}{Polygenic Risk Scores}{subsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}LDPred-funct}{17}{subsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces {\bf  Calculated Polygenic component of Schizophrenia and Bipolar disorder, as well as non-psychiatric\cite  {Consortium2009}} Calculated $R^2$ for Schizophrenia, Bipolar Disorder using different SNP thresholds. On the right are some common diseases. CAD, coronary artery disease; CD, Crohn\IeC {\textquoteright }s disease; HT, hypertension; RA, rheumatoid arthritis; T1D, type I diabetes; T2D, type II diabetes.}}{18}{figure.2.3}\protected@file@percent }
\newlabel{prs1}{{2.3}{18}{{\bf Calculated Polygenic component of Schizophrenia and Bipolar disorder, as well as non-psychiatric\cite {Consortium2009}} Calculated $R^2$ for Schizophrenia, Bipolar Disorder using different SNP thresholds. On the right are some common diseases. CAD, coronary artery disease; CD, Crohnâ€™s disease; HT, hypertension; RA, rheumatoid arthritis; T1D, type I diabetes; T2D, type II diabetes}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces {\bf  Accuracy of PRS prediction methods on different UK BioBank phenotypes\cite  {Marquez-Luna375337}} $R^2$ Results using 5 different polygenic methods using the UK BioBank dataset and evaluating 16 different phenotypes. The $R^2$ value is compared against the maximum calculated heritability value at the top of the graphs.}}{18}{figure.2.4}\protected@file@percent }
\newlabel{ldpred1}{{2.4}{18}{{\bf Accuracy of PRS prediction methods on different UK BioBank phenotypes\cite {Marquez-Luna375337}} $R^2$ Results using 5 different polygenic methods using the UK BioBank dataset and evaluating 16 different phenotypes. The $R^2$ value is compared against the maximum calculated heritability value at the top of the graphs}{figure.2.4}{}}
\citation{Ballard2011}
\citation{Mudher2002}
\citation{Mudher2002}
\citation{Mudher2002}
\citation{Scheltens2016}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Alzheimer Disease}{19}{section.2.2}\protected@file@percent }
\newlabel{betaAmyloid}{{2.2}{19}{Alzheimer Disease}{section.2.2}{}}
\citation{Scheltens2016}
\citation{Scheltens2016}
\citation{KARCH201543}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces {\bf  Plaque formation and possible causes\cite  {Mudher2002}} Development of Alzheimer's disease relating to the procedure via which the APP protein converts into Amyloid plaques that cause the disease. On the left are some possible environmental influences including the APOE $\epsilon 4$ gene}}{20}{figure.2.5}\protected@file@percent }
\newlabel{gr1}{{2.5}{20}{{\bf Plaque formation and possible causes\cite {Mudher2002}} Development of Alzheimer's disease relating to the procedure via which the APP protein converts into Amyloid plaques that cause the disease. On the left are some possible environmental influences including the APOE $\epsilon 4$ gene}{figure.2.5}{}}
\citation{KARCH201543}
\citation{KARCH201543}
\citation{McKhann2011}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces {\bf  Genetic variants that contribute to Alzheimer's Disease\cite  {KARCH201543}} These are some of the genes as well as the biological functions they play that increase the risk of developing Alzheimer's Disease}}{21}{figure.2.6}\protected@file@percent }
\newlabel{genAD}{{2.6}{21}{{\bf Genetic variants that contribute to Alzheimer's Disease\cite {KARCH201543}} These are some of the genes as well as the biological functions they play that increase the risk of developing Alzheimer's Disease}{figure.2.6}{}}
\citation{Boser:1992:TAO:130385.130401}
\citation{svm1}
\citation{svm1}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Machine Learning}{23}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Support Vector Machines}{23}{subsection.2.3.1}\protected@file@percent }
\newlabel{RF}{{2.3.1}{23}{Support Vector Machines}{subsection.2.3.1}{}}
\citation{598994}
\citation{rf}
\citation{rf}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces {\bf  Support Vector Machine Hyper-plane classification \cite  {svm1}} An example of the hyper-plane classification being done by a SVM. The data samples from different classes nearby are chosen as Support Vectors and a hyper-plane is chosen which maximizes the margin between classes.}}{24}{figure.2.7}\protected@file@percent }
\newlabel{svm1}{{2.7}{24}{{\bf Support Vector Machine Hyper-plane classification \cite {svm1}} An example of the hyper-plane classification being done by a SVM. The data samples from different classes nearby are chosen as Support Vectors and a hyper-plane is chosen which maximizes the margin between classes}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Random Forest}{24}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces {\bf  Random Forest Structure\cite  {rf}} The diagram shows an example of the structure a Random Forest follows, with different trees each using a subset of random features.}}{25}{figure.2.8}\protected@file@percent }
\newlabel{rf1}{{2.8}{25}{{\bf Random Forest Structure\cite {rf}} The diagram shows an example of the structure a Random Forest follows, with different trees each using a subset of random features}{figure.2.8}{}}
\citation{Rosenblatt58theperceptron:}
\citation{percep}
\citation{percep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Neural Networks and Deep Neural Networks}{26}{subsection.2.3.3}\protected@file@percent }
\newlabel{DNNs}{{2.3.3}{26}{Neural Networks and Deep Neural Networks}{subsection.2.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces {\bf  Perceptron Model\cite  {percep}} The model describes a simple single-layer perceptron with the inputs on the left, followed by the weights by which to multiply them, the sum done in them and finally the activation function that leads to the output. The feedback error to guide the learning process is also shown.}}{26}{figure.2.9}\protected@file@percent }
\newlabel{percep1}{{2.9}{26}{{\bf Perceptron Model\cite {percep}} The model describes a simple single-layer perceptron with the inputs on the left, followed by the weights by which to multiply them, the sum done in them and finally the activation function that leads to the output. The feedback error to guide the learning process is also shown}{figure.2.9}{}}
\citation{LeCun2015}
\citation{LeCun2015}
\citation{LeCun2015}
\citation{Schmidhuber2015}
\citation{Nair:2010:RLU:3104322.3104425}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces {\bf  Neural Networks, Forward-propagation and Backpropagation\cite  {LeCun2015}} a) shows how MLP can make an input linearly separable by transforming the input space. b) shows the backpropagation error equation in terms of derivatives. c) gives the forward-propagation equations in a sample DNN, while d) describes the backpropagation }}{28}{figure.2.10}\protected@file@percent }
\newlabel{dnn1}{{2.10}{28}{{\bf Neural Networks, Forward-propagation and Backpropagation\cite {LeCun2015}} a) shows how MLP can make an input linearly separable by transforming the input space. b) shows the backpropagation error equation in terms of derivatives. c) gives the forward-propagation equations in a sample DNN, while d) describes the backpropagation}{figure.2.10}{}}
\citation{DBLP:journals/corr/KingmaB14}
\citation{JMLR:v15:srivastava14a}
\citation{Ng:2004:FSL:1015330.1015435}
\citation{Ioffe2015BatchNA}
\citation{fresa}
\citation{fresa}
\citation{fresa}
\citation{fresa}
\citation{fresa}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}FRESA.CAD Benchmark}{31}{subsection.2.3.4}\protected@file@percent }
\newlabel{fresaCAD}{{2.3.4}{31}{FRESA.CAD Benchmark}{subsection.2.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces {\bf  Repeated Holdout Cross-Validation with FRESA.CAD\cite  {fresa}} The flow diagram depicts the RHCV procedure implemented in the FRESA.CAD benchmark used to split the input dataset for training and validation}}{32}{figure.2.11}\protected@file@percent }
\newlabel{fresafig1}{{2.11}{32}{{\bf Repeated Holdout Cross-Validation with FRESA.CAD\cite {fresa}} The flow diagram depicts the RHCV procedure implemented in the FRESA.CAD benchmark used to split the input dataset for training and validation}{figure.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces {\bf  FRESA.CAD Benchmark procedure\cite  {fresa}} The flow diagram depicts the Benchmarking procedure across the different models and filters for comparison in each Cross-Validation iteration}}{33}{figure.2.12}\protected@file@percent }
\newlabel{fresafig2}{{2.12}{33}{{\bf FRESA.CAD Benchmark procedure\cite {fresa}} The flow diagram depicts the Benchmarking procedure across the different models and filters for comparison in each Cross-Validation iteration}{figure.2.12}{}}
\citation{Zou2019}
\citation{Sundaram2018}
\citation{Sundaram2018}
\citation{Sundaram2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Previous Work}{34}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Genetic Prediction with Deep Learning}{34}{subsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces {\bf  Deep learning Architecture and results\cite  {Sundaram2018}} Image a) describes the deep network architecture to predict pathogenicity depending on the given Amino Acid sequence variant, the reference and the support network results. Image b) describes the structure of the support deep networks for predicting the secondary structure and the solvent accesibility. Image c) shows the predicted pathogenicity given to mutations at specific points, as well as the reported ClinVar pathogenicity. Image d) e) and f) give the results obtain with respect to withheld variants, missense variants and classification performance}}{35}{figure.2.13}\protected@file@percent }
\newlabel{predHum1}{{2.13}{35}{{\bf Deep learning Architecture and results\cite {Sundaram2018}} Image a) describes the deep network architecture to predict pathogenicity depending on the given Amino Acid sequence variant, the reference and the support network results. Image b) describes the structure of the support deep networks for predicting the secondary structure and the solvent accesibility. Image c) shows the predicted pathogenicity given to mutations at specific points, as well as the reported ClinVar pathogenicity. Image d) e) and f) give the results obtain with respect to withheld variants, missense variants and classification performance}{figure.2.13}{}}
\citation{Zhou2017}
\citation{Zhou2017}
\citation{Zhou2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces {\bf  Overview of the prediction Architecture\cite  {Zhou2017}} The diagram describes the schematic overview of the system. The Deep network receives the sequence at the genetic variant, from which it predicts chromatin profiles contrasts it between the reference and the variant and thus obtains a prediction on the functional effects.}}{36}{figure.2.14}\protected@file@percent }
\newlabel{predeff1}{{2.14}{36}{{\bf Overview of the prediction Architecture\cite {Zhou2017}} The diagram describes the schematic overview of the system. The Deep network receives the sequence at the genetic variant, from which it predicts chromatin profiles contrasts it between the reference and the variant and thus obtains a prediction on the functional effects}{figure.2.14}{}}
\citation{Mishra2017}
\citation{Orru2012}
\citation{Kloppel2008}
\citation{SANKARI2011165}
\citation{Suk2014}
\citation{Suk2014}
\citation{Suk2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Alzheimer Disease Detection}{37}{subsection.2.4.2}\protected@file@percent }
\citation{Liu2015}
\citation{Liu2015}
\citation{Liu2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces {\bf  Suk et al proposed method for deep learning\cite  {Suk2014}}{ Method via which they create a feature representation model using deep learning by extracting patches from MRI and PET images which they process and then feed into a Deep Boltzmann Machine, the outputs of which are then classified using a SVM. }}}{38}{figure.2.15}\protected@file@percent }
\newlabel{gr2}{{2.15}{38}{{\bf Suk et al proposed method for deep learning\cite {Suk2014}}{ Method via which they create a feature representation model using deep learning by extracting patches from MRI and PET images which they process and then feed into a Deep Boltzmann Machine, the outputs of which are then classified using a SVM. }}{figure.2.15}{}}
\citation{Rowe2013}
\citation{Mathotaarachchi2017a}
\citation{Long2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Alzheimer Prediction}{39}{subsection.2.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces {\bf  Architectures from Liu et al\cite  {Liu2015}}{ The architecture used to train Deep Learning architectures, first being single-modal and afterwards with the better multi-modal structure to predict the 4 classes.}}}{40}{figure.2.16}\protected@file@percent }
\newlabel{gr3}{{2.16}{40}{{\bf Architectures from Liu et al\cite {Liu2015}}{ The architecture used to train Deep Learning architectures, first being single-modal and afterwards with the better multi-modal structure to predict the 4 classes.}}{figure.2.16}{}}
\@setckpt{chapters/content/background}{
\setcounter{page}{41}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{26}
\setcounter{section@level}{2}
}
